import psycopg2
import boto3
def main_redshift_creds(current_query,fetch,username, password):
      conn_string = "host='"
      conn = psycopg2.connect(conn_string)
      conn.autocommit=True
      cursor = conn.cursor()
      cursor.execute(current_query)

      if fetch==1:
          records=cursor.fetchall()
          conn.commit()
          return records
        # retrieve the records from the database
      cursor.close()
      conn.close()
    
def upload_data_to_redshift_creds(filename, tablename,username, password):
       print ('Creating S3 Connection')
       s3 = boto3.resource('s3')
       print ('Copying local file to S3')
       data = open(filename, 'rb')
       s3.Bucket('').put_object(Key='avenkata/staging/temp_x', Body=data,ServerSideEncryption='AES256')
       print ('Local to S3 Transfer successful')
       
       main_redshift_creds( "COPY "+ str(tablename)+ "  FROM   ')


       s3.Bucket('').delete_objects(
          Delete={
             'Objects': [
                 {
                     'Key': 'avenkata/staging/temp_x',

                 },
             ],

         }
         )
       print ('Staging File Deleted')
       
import os
os.chdir('')
print(os.getcwd())

import pandas as pd
def upload_csv():



upload_csv()

